{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from random import sample\n",
    "from collections import defaultdict\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "from sklearn.feature_selection import f_classif, mutual_info_classif, RFE\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_in_data(threshold=0.001, window=5, drop_lags=True, drop_2020=True):\n",
    "    \"\"\"\n",
    "    Takes in an ITM threshold and window (1, 5, 10, 21, reads in and returns data clean.)\n",
    "    \"\"\"\n",
    "    badf = pd.read_csv('data/badf.csv', index_col=0, parse_dates=True)\n",
    "    badf.drop(columns = ['open', 'high', 'low', 'close'], inplace=True)\n",
    "\n",
    "    badf['label'] = (badf[f'target_{window}d']>threshold).astype(int)\n",
    "    badf = badf.drop(columns = ['target_1d','target_5d','target_10d','target_21d'])\n",
    "    badf = badf.drop(columns = [column for column in badf.columns if 'fb' in column])\n",
    "    badf = badf.drop(columns = [column for column in badf.columns if 'tsla' in column])\n",
    "    \n",
    "    # Drop lags, too many datapoints right now\n",
    "    if drop_lags:\n",
    "        badf = badf.drop(columns = [column for column in badf.columns if '_lag' in column])\n",
    "\n",
    "    for column in badf.columns:\n",
    "        badf[column] = badf[column].fillna(badf[column].median())\n",
    "\n",
    "    if drop_2020:\n",
    "        badf = badf[badf.index<'2020-01-01']\n",
    "        badf_test_holdout = badf[badf.index >= '2019-01-01']\n",
    "        badf = badf[badf.index < '2019-01-01']\n",
    "    else:\n",
    "        badf_test_holdout = badf[badf.index >= '2020-01-01']\n",
    "        badf = badf[badf.index < '2020-01-01']        \n",
    "    \n",
    "    return badf, badf_test_holdout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into features and target, split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_features_target(df):\n",
    "    X = df.drop('label', axis=1)\n",
    "    y = df['label']\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_training_test(X, y, train_prop=0.8):\n",
    "    split_index = int(round(train_prop*len(X)))\n",
    "    \n",
    "    X_train = X[:split_index]\n",
    "    X_test = X[split_index:]\n",
    "\n",
    "    y_train = y[:split_index]\n",
    "    y_test = y[split_index:]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline model time\n",
    "\n",
    "Will use this to compare everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decision_tree_auc(X, y):\n",
    "    \"\"\"\n",
    "    Basic decision tree, return AUC and confusion matrix\n",
    "    \"\"\"\n",
    "\n",
    "    X_train, X_test, y_train, y_test = split_training_test(X, y)\n",
    "\n",
    "    baseline_tree = DecisionTreeClassifier().fit(X_train, y_train)\n",
    "\n",
    "    y_pred = baseline_tree.predict_proba(X_test)[:,1]\n",
    "\n",
    "    return roc_auc_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low variance filter\n",
    "\n",
    "One strategy to reduce dimensionality is to drop features with little or no variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_variances(X):\n",
    "    variances = X.var()\n",
    "    return variances.sort_values()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's difficult to say what \"too low\" variance is, so lets just drop all the rows with a variance of 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_variance(X, y, hp=None):\n",
    "    if hp is not None:\n",
    "        threshold = hp['threshold']\n",
    "    else:\n",
    "        threshold = 0\n",
    "    variances = show_variances(X)\n",
    "    return X[[col for col in X if variances[col] > threshold]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F classification and Mutual Information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_scores_ranking(X, y):\n",
    "    \"\"\"\n",
    "    Use F-Classification to rank features (not too sure what this means yet)\n",
    "    \"\"\"\n",
    "    \n",
    "    f_scores = f_classif(X, y)[0]\n",
    "\n",
    "    f_features = pd.Series(dict(zip(X.columns, f_scores)))\n",
    "\n",
    "    return f_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mutual_information_ranking(X, y):\n",
    "    \"\"\"\n",
    "    Use mutual information to rank features by importance\n",
    "    \"\"\"\n",
    "    \n",
    "    mi_scores = mutual_info_classif(X, y)\n",
    "\n",
    "    mi_features = pd.Series(dict(zip(X.columns, mi_scores)))\n",
    "\n",
    "    return mi_features.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_mutual_information(X, y, hp=None):\n",
    "    \"\"\"\n",
    "    Use mutual information to filter out useless features\n",
    "    \"\"\"\n",
    "    threshold=0\n",
    "    if hp is not None:\n",
    "        threshold = hp['threshold']\n",
    "    mi_scores = mutual_information_ranking(X, y)\n",
    "    return X[[col for col in X if mi_scores[col] > threshold]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a LOT of features that provide 0 mutual information with the target variable, we can drop them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Correlations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(X):\n",
    "    corr = X.corr()\n",
    "    sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not really too helpful lol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pairwise_correlation_ranking(X, threshold):\n",
    "    \"\"\"\n",
    "    Returns pairwise correlation of features ranked by absolute value, if above a certain threshold\n",
    "    \"\"\"\n",
    "    \n",
    "    corr = X.corr()\n",
    "    \n",
    "    corrdict = {}\n",
    "    for i in range(len(corr)):\n",
    "        for j in range(len(corr.columns)):\n",
    "            if i != j and np.abs(corr.iloc[i,j] > threshold):\n",
    "                corrdict[tuple(sorted([corr.columns[i], corr.columns[j]]))] = corr.iloc[i,j]\n",
    "    return np.array(sorted(corrdict.items(), key=lambda x: np.abs(x[1]), reverse=True), dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_correlation(X, y, hp=None):\n",
    "    \"\"\"\n",
    "    Greedy filtering of features by correlation\n",
    "    \"\"\"\n",
    "    threshold = 0.8\n",
    "    if hp is not None:\n",
    "        threshold = hp['threshold']\n",
    "    correlations = pairwise_correlation_ranking(X, threshold)\n",
    "    while len(correlations) > 0:\n",
    "        worst_feature = mutual_information_ranking(X[list(correlations[0][0])], y).idxmin()\n",
    "        X.drop(worst_feature, axis=1, inplace=True)\n",
    "        correlations = pairwise_correlation_ranking(X, threshold)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pick best feature out of list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pick_best_n_features(X, y, metric = 'MI', n_features = 1):\n",
    "    \"\"\"\n",
    "    Returns the column name of the \"best\" feature, determined by either mutual information (MI) or F-score (F)\n",
    "    \"\"\"\n",
    "    \n",
    "    if metric == 'MI':\n",
    "        ranking = mutual_information_ranking(X, y)\n",
    "    elif metric == 'corr':\n",
    "        ranking = X.corrwith(y).sort_values(ascending=False)\n",
    "    \n",
    "    return ranking.index[:n_features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter by best window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_by_best_window(X, y, hp=None):\n",
    "    \"\"\"\n",
    "    Filters features by picking the best window for each ticker\n",
    "    \"\"\"\n",
    "    \n",
    "    metric = 'MI'\n",
    "    if hp is not None:\n",
    "        metric = hp['metric']\n",
    "    \n",
    "    tickers = ['dxy', 'eem', 'aapl', 'amzn', 'msft', 'gld', 'tnx', 'vix', 'jnk']\n",
    "    for ticker in tickers:\n",
    "        cols = [col for col in X if ticker in col]\n",
    "        if len(cols) > 0:\n",
    "            X_sub = X[[col for col in X if ticker in col]]\n",
    "            best = pick_best_n_features(X_sub, y, metric=metric)[0]\n",
    "            X.drop(columns=[col for col in X_sub if col != best], inplace=True)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-best feature selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def k_best_feature_selection(X, y, hp=None ):\n",
    "    \"\"\"\n",
    "    Pull the top n features using a simple metric evaluation\n",
    "    \"\"\"\n",
    "    metric = 'MI'\n",
    "    n_features=10\n",
    "    \n",
    "    if hp is not None:\n",
    "        metric = hp['metric']\n",
    "        n_features = hp['n_features']\n",
    "    \n",
    "    top_n = pick_best_n_features(X, y, metric, n_features)\n",
    "    return X[[col for col in X if col in list(top_n)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recursive feature elimination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rfe_feature_selection(X, y, hp=None):\n",
    "    \"\"\"\n",
    "    Uses sklearn's recursive feature elimination tool to pull top n features.\n",
    "    \"\"\"\n",
    "    n_features=10\n",
    "    \n",
    "    if hp is not None:\n",
    "        n_features = hp['n_features']\n",
    "    \n",
    "    estimator = DecisionTreeClassifier()\n",
    "    selector = RFE(estimator, n_features_to_select=n_features, step=1)\n",
    "\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    rankings = pd.Series(dict(zip(X.columns, selector.ranking_)))\n",
    "\n",
    "    return X[[col for col in X if rankings[col] == 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_forest_feature_selection(X, y, hp=None):\n",
    "    \"\"\"\n",
    "    Use a random forest to select the n most important features.\n",
    "    \"\"\"\n",
    "    n_features = 10\n",
    "    \n",
    "    if hp is not None:\n",
    "        n_features = hp['n_features']\n",
    "    \n",
    "    rf = RandomForestClassifier(n_estimators=100)\n",
    "    \n",
    "    rf.fit(X, y)\n",
    "    \n",
    "    top_n = pd.Series(dict(zip(X.columns, rf.feature_importances_))).sort_values(ascending=False)[:n_features]\n",
    "    \n",
    "    return X[[col for col in X if col in list(top_n.index)]]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Great AUC bakeoff\n",
    "\n",
    "I have no clue how to set hyperparameters for feature selection, so we're gonna try them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "binary_thresholds = [0, 0.0005, 0.001]\n",
    "window = [1, 5, 10, 21]\n",
    "\n",
    "initial_filtering = [None, None, None, None, 'filter_by_variance', \n",
    "                     'filter_by_mutual_information', 'filter_by_correlation', \n",
    "                     'filter_by_best_window']\n",
    "\n",
    "feature_selection = ['k_best_feature_selection', 'rfe_feature_selection', 'random_forest_feature_selection']\n",
    "\n",
    "models = {\n",
    "    'filter_by_variance': filter_by_variance,\n",
    "    'filter_by_mutual_information': filter_by_mutual_information,\n",
    "    'filter_by_correlation': filter_by_correlation,\n",
    "    'filter_by_best_window': filter_by_best_window,\n",
    "    'k_best_feature_selection': k_best_feature_selection,\n",
    "    'rfe_feature_selection': rfe_feature_selection,\n",
    "    'random_forest_feature_selection': random_forest_feature_selection\n",
    "}\n",
    "\n",
    "hyperparams = {\n",
    "    'filter_by_variance': {\n",
    "        'threshold': [0, 1e-4, 1e-3, 1e-2, 1e-1]\n",
    "    },\n",
    "    'filter_by_mutual_information': {\n",
    "        'threshold': [0, 0.001, 0.002, 0.005, 0.01]\n",
    "    },\n",
    "    'filter_by_correlation': {\n",
    "        'threshold': [0.6, 0.7, 0.8, 0.9]\n",
    "    },\n",
    "    'filter_by_best_window': {\n",
    "        'metric': ['MI', 'corr']\n",
    "    },\n",
    "    'k_best_feature_selection': {\n",
    "        'metric': ['MI', 'corr'],\n",
    "        'n_features': [5, 8, 10, 15, 20]\n",
    "    },\n",
    "    'rfe_feature_selection': {\n",
    "        'n_features': [5, 8, 10, 15, 20],\n",
    "    },\n",
    "    'random_forest_feature_selection': {\n",
    "        'n_features': [5, 8, 10, 15, 20],\n",
    "    }\n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First bakeoff: choose threshold and target window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New high AUC for 1_day: 0.48726941590747563\n",
      "New high AUC for 1_day: 0.5147133649932157\n",
      "New high AUC for 1_day: 0.5344040061608168\n",
      "New high AUC for 1_day: 0.5386164792620793\n",
      "Done!\n",
      "New high AUC for 10_day: 0.5184924297627722\n",
      "New high AUC for 10_day: 0.5464780179742531\n",
      "New high AUC for 10_day: 0.5556371953687961\n",
      "Done!\n",
      "New high AUC for 21_day: 0.49797588500535606\n",
      "New high AUC for 21_day: 0.534308230155369\n",
      "New high AUC for 21_day: 0.5437336119305146\n",
      "New high AUC for 21_day: 0.5438579008993298\n",
      "New high AUC for 21_day: 0.5948759788412512\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "if os.path.exists('setups.csv'):\n",
    "    setup_df = pd.read_csv('setups.csv', index_col=0)\n",
    "else:\n",
    "    setup_df = pd.DataFrame()\n",
    "\n",
    "if os.path.exists('feature_counts.csv'):\n",
    "    feature_counts = pd.read_csv('feature_counts.csv', index_col=0)\n",
    "else:\n",
    "    feature_counts = pd.DataFrame()\n",
    "\n",
    "for win in window:\n",
    "    for i in range(100):\n",
    "        bin_thresh = sample(binary_thresholds, 1)[0]\n",
    "        initial_filters = sample(initial_filtering, 3)\n",
    "        feature_selector = sample(feature_selection, 1)[0]\n",
    "\n",
    "        hps = {}\n",
    "\n",
    "        for model in initial_filters + [feature_selector]:\n",
    "            if model is not None:\n",
    "                hps[model] = {key: sample(value, 1)[0] for key, value in hyperparams[model].items()}\n",
    "\n",
    "        data, holdout = read_in_data(threshold=bin_thresh, window=win)\n",
    "\n",
    "        X, y = split_features_target(data)\n",
    "\n",
    "        for fil in initial_filters:\n",
    "            if fil is not None:\n",
    "                X = models[fil](X, y, hp=hps[fil])\n",
    "\n",
    "        X = models[feature_selector](X, y, hp=hps[feature_selector])\n",
    "\n",
    "        features = X.columns\n",
    "        for f in features:\n",
    "            if f'{win}_day' in feature_counts.columns and f in feature_counts.index:\n",
    "                feature_counts.at[f, f'{win}_day'] += 1\n",
    "            else:\n",
    "                feature_counts.at[f, f'{win}_day'] = 1\n",
    "\n",
    "        auc = decision_tree_auc(X, y)\n",
    "        new_high = False\n",
    "        if f'{win}_day' not in setup_df.columns or 'AUC' not in setup_df.index:\n",
    "            new_high = True\n",
    "        else: \n",
    "            if auc > float(setup_df.at['AUC', f'{win}_day']):\n",
    "                new_high = True\n",
    "        if new_high:\n",
    "            print(f'New high AUC for {win}_day: {auc}')\n",
    "\n",
    "            setup = {\n",
    "                'AUC': auc,\n",
    "                'binary_threshold': bin_thresh,\n",
    "                'initial_filters': initial_filters,\n",
    "                'feature_selection': feature_selector,\n",
    "                'hyperparameters': hps,\n",
    "                'features': features\n",
    "            }\n",
    "\n",
    "            setup_df[f'{win}_day'] = pd.Series(setup)\n",
    "            setup_df.to_csv('setups.csv')\n",
    "\n",
    "        feature_counts.to_csv('feature_counts.csv')\n",
    "print('Done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished 0!\n",
      "Finished 100!\n",
      "Finished 200!\n",
      "Finished 300!\n",
      "Finished 400!\n",
      "Finished 500!\n",
      "Finished 600!\n",
      "Finished 700!\n",
      "Finished 800!\n",
      "Finished 900!\n",
      "Finished 1000!\n",
      "Finished 1100!\n",
      "Finished 1200!\n",
      "Finished 1300!\n",
      "Finished 1400!\n",
      "Finished 1500!\n",
      "Finished 1600!\n",
      "Finished 1700!\n",
      "Finished 1800!\n",
      "Finished 1900!\n",
      "Finished 2000!\n",
      "Finished 2100!\n",
      "Finished 2200!\n",
      "Finished 2300!\n",
      "Finished 2400!\n",
      "Finished 2500!\n",
      "Finished 2600!\n",
      "Finished 2700!\n",
      "Finished 2800!\n",
      "Finished 2900!\n",
      "Finished 3000!\n",
      "Finished 3100!\n",
      "Finished 3200!\n",
      "Finished 3300!\n",
      "Finished 3400!\n",
      "Finished 3500!\n",
      "Finished 3600!\n",
      "Finished 3700!\n",
      "Finished 3800!\n",
      "Finished 3900!\n",
      "Finished 4000!\n",
      "Finished 4100!\n",
      "Finished 4200!\n",
      "Finished 4300!\n",
      "Finished 4400!\n",
      "Finished 4500!\n",
      "Finished 4600!\n",
      "Finished 4700!\n",
      "Finished 4800!\n",
      "Finished 4900!\n",
      "Finished 5000!\n",
      "Finished 5100!\n",
      "Finished 5200!\n",
      "Finished 5300!\n",
      "Finished 5400!\n",
      "Finished 5500!\n",
      "Finished 5600!\n",
      "Finished 5700!\n",
      "Finished 5800!\n",
      "Finished 5900!\n",
      "Finished 6000!\n",
      "Finished 6100!\n",
      "Finished 6200!\n",
      "Finished 6300!\n",
      "Finished 6400!\n",
      "Finished 6500!\n",
      "Finished 6600!\n",
      "Finished 6700!\n",
      "Finished 6800!\n",
      "Finished 6900!\n",
      "Finished 7000!\n",
      "Finished 7100!\n",
      "Finished 7200!\n",
      "Finished 7300!\n",
      "Finished 7400!\n",
      "Finished 7500!\n",
      "Finished 7600!\n",
      "Finished 7700!\n",
      "Finished 7800!\n",
      "Finished 7900!\n",
      "Finished 8000!\n",
      "Finished 8100!\n",
      "Finished 8200!\n",
      "Finished 8300!\n",
      "Finished 8400!\n",
      "Finished 8500!\n",
      "Finished 8600!\n",
      "Finished 8700!\n",
      "Finished 8800!\n",
      "Finished 8900!\n",
      "Finished 9000!\n",
      "Finished 9100!\n",
      "Finished 9200!\n",
      "Finished 9300!\n",
      "Finished 9400!\n",
      "Finished 9500!\n",
      "Finished 9600!\n",
      "Finished 9700!\n",
      "Finished 9800!\n"
     ]
    }
   ],
   "source": [
    "feature_counts = defaultdict(int)\n",
    "if os.path.exists('feature_counts_10d.csv'):\n",
    "    feature_counts = pd.read_csv('feature_counts_10d.csv', index_col=0, header=None)[1].to_dict(into=feature_counts)\n",
    "\n",
    "for i in range(9900):\n",
    "    bin_thresh = 0.001\n",
    "    win = 10\n",
    "    initial_filters = ['filter_by_best_window', 'filter_by_mutual_information']\n",
    "    feature_selector = sample(feature_selection, 1)[0]\n",
    "\n",
    "    hps = {}\n",
    "\n",
    "    for model in initial_filters + [feature_selector]:\n",
    "        if model is not None:\n",
    "            hps[model] = {key: sample(value, 1)[0] for key, value in hyperparams[model].items()}\n",
    "\n",
    "    data, holdout = read_in_data(threshold=bin_thresh, window=win)\n",
    "\n",
    "    X, y = split_features_target(data)\n",
    "\n",
    "    for fil in initial_filters:\n",
    "        if fil is not None:\n",
    "            X = models[fil](X, y, hp=hps[fil])\n",
    "    \n",
    "    if len(X.columns) > hps[feature_selector]['n_features']:\n",
    "        X = models[feature_selector](X, y, hp=hps[feature_selector])\n",
    "\n",
    "    features = X.columns\n",
    "    for f in features:\n",
    "        feature_counts[f] += 1\n",
    "    if i % 10 == 0:\n",
    "        dat = pd.Series(feature_counts).sort_values(ascending=False).to_csv('feature_counts_10d.csv', header=None)\n",
    "    if i % 100 == 0:\n",
    "        print(f'Finished {i}!')\n",
    "print('Done!')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
